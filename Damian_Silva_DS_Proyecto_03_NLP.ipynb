{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Damian Silva DS_Proyecto_03_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnbqxDKK-b9H"
      },
      "source": [
        "Archivos:\n",
        "https://drive.google.com/file/d/1Z808iMAlYU8GEUmhb5O58rpFoYEgIyYH/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/17peZoammCnbLdeht-IDql9oSzHFbsGIe/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/1JZ6BwKhQdUT5KPbAEixkGQWmlfEKhcPd/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GRQnxMzISE_"
      },
      "source": [
        "# Proyecto 03 - Procesamiento del Lenguaje Natural\n",
        "\n",
        "## Base de datos: The Multilingual Amazon Reviews Corpus\n",
        "\n",
        "**Recuerda descargar el data de [aquí](https://github.com/kang205/SASRec). Es un archivo .zip que contiene tres documentos. Más información sobre el data [aquí](https://registry.opendata.aws/amazon-reviews-ml/). Es importante que tengas en cuenta la [licencia](https://docs.opendata.aws/amazon-reviews-ml/license.txt) de este data.**\n",
        "\n",
        "### Exploración de datos y Procesamiento del Lenguaje Natural\n",
        "\n",
        "Dedícale un buen tiempo a hacer un Análisis Exploratorio de Datos. Considera que hasta que no hayas aplicado las herramientas de Procesamiento del Lenguaje Natural vistas, será difícil completar este análisis. Elige preguntas que creas que puedas responder con este data. Por ejemplo, ¿qué palabras están asociadas a calificaciones positivas y qué palabras a calificaciones negativas?\n",
        "\n",
        "### Machine Learning\n",
        "\n",
        "Implementa un modelo que, dada la crítica de un producto, asigne la cantidad de estrellas correspondiente. **Para pensar**: ¿es un problema de Clasificación o de Regresión?\n",
        "\n",
        "1. Haz todas las transformaciones de datos que consideres necesarias. Justifica.\n",
        "1. Evalúa de forma apropiada sus resultados. Justifica la métrica elegida.\n",
        "1. Elige un modelo benchmark y compara tus resultados con este modelo.\n",
        "1. Optimiza los hiperparámetros de tu modelo.\n",
        "1. Intenta responder la pregunta: ¿Qué información está usando el modelo para predecir?\n",
        "\n",
        "**Recomendación:** si no te resulta conveniente trabajar en español con NLTK, te recomendamos que explores la librería [spaCy](https://spacy.io/).\n",
        "\n",
        "### Para pensar, investigar y, opcionalmente, implementar\n",
        "1. ¿Valdrá la pena convertir el problema de Machine Learning en un problema binario? Es decir, asignar únicamente las etiquetas Positiva y Negativa a cada crítica y hacer un modelo que, en lugar de predecir las estrellas, prediga esa etiqueta. Pensar en qué situación puede ser útil. ¿Esperas que el desempeño sea mejor o peor?\n",
        "1. ¿Hay algo que te gustaría investigar o probar?\n",
        "\n",
        "### **¡Tómate tiempo para investigar y leer mucho!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX6EBB336cRd"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1GFwraSISFB"
      },
      "source": [
        "# Vincular la cuenta de Google Drive donde están almacenados los archivos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOOaGdy-6V7b"
      },
      "source": [
        "data = pd.read_json(\"/content/gdrive/MyDrive/dataset_es_train.json\", lines= True)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKSRYtAWVwen"
      },
      "source": [
        "len(data['product_id'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-wo0BJBWYEj"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoe4efTbyZoM"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IUCUF-scsH2"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cgBWRVJycud"
      },
      "source": [
        "# Observo los tipos de tados por variable\n",
        "data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLyM6dmiygTO"
      },
      "source": [
        "# Calculo los estadísticos principales (sólo tenemos una variable numérica)\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSzTZ9r1yjVF"
      },
      "source": [
        "# CALCULO MISSING\n",
        "\n",
        "a = data[['review_body','review_title', 'product_category', 'stars']].isnull().sum(axis=0)\n",
        "b = round(a/data.shape[0]*100,2)\n",
        "\n",
        "missing_df = pd.DataFrame({'missing_totales' : a, 'missing_freq' : b})\n",
        "\n",
        "missing_df[missing_df['missing_totales']>0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WouWk37iymDB"
      },
      "source": [
        "La base de datos no presenta valores faltantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6KBwc9GyqkU"
      },
      "source": [
        "## Distribucion de las reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbnKiZSXypPX"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(5, 5))\n",
        "\n",
        "# Grafico\n",
        "ax = sns.countplot(data = data, x = 'stars', orient=\"v\", palette ='Set3')\n",
        "\n",
        "plt.title('Cantidad Reviews por Stars')\n",
        "plt.xlabel('Star')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "    \n",
        "#ax.yaxis.set_ticks(np.linspace(0, len_star, 10))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibUgAqsMy1FT"
      },
      "source": [
        "Con el presente gráfico podemos observar que cada una de nuestra clases se encuentran balanceadas, presentando 40 mil instancias por cantidad de estrellas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhXviYoCzDwW"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(20, 10))\n",
        "\n",
        "# Grafico\n",
        "ax = sns.countplot(data = data, x = 'product_category', order= data['product_category'].value_counts().index, orient=\"v\", palette ='Set3')\n",
        "plt.setp(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "plt.title('Cantidad Reviews por Categoría del Procuto')\n",
        "plt.xlabel('Star')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzMcvzIJzHsg"
      },
      "source": [
        "#Preparo los datos\n",
        "\n",
        "N_TOP = 10\n",
        "\n",
        "# Preparo los datos para graficar\n",
        "\n",
        "count = data.groupby(['stars','product_category'], as_index=False)['language'].count()\n",
        "count_max = count.sort_values(['stars', 'language'], ascending=False).groupby('stars').head(N_TOP)\n",
        "\n",
        "# Renombro\n",
        "count_max = count_max.rename(columns = {'language': 'Total'}, inplace = False)\n",
        "\n",
        "# Joineo con la data original para quedarme sólo con los barrios que están en el top\n",
        "data_grap = data.merge(count_max, how='left', on=['stars','product_category'])\n",
        "\n",
        "#Reemplazo nuelos por 0\n",
        "data_grap = data_grap.fillna(0)\n",
        "\n",
        "# Reemplazo por OTROS para graficar\n",
        "data_grap['product_category']= np.where(data_grap['Total'] == 0,'OTROS', data_grap['product_category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q2KlOGpzOea"
      },
      "source": [
        "data_grap.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsJLSJf9zTX9"
      },
      "source": [
        "# Grafico\n",
        "\n",
        "# 1 ESTRELLA #######################################\n",
        "plt.figure(figsize=(11,14))\n",
        "plt.subplot(5, 1, 1)\n",
        "\n",
        "g1 = sns.countplot(data = data_grap[data_grap['stars']==1], x='product_category', \n",
        "                   order = data_grap[data_grap['stars']==1]['product_category'].value_counts().index.drop('OTROS').insert(N_TOP+1, 'OTROS'), \n",
        "                   orient=\"v\", palette ='Set3')\n",
        "plt.setp(g1.get_xticklabels(), rotation=90)\n",
        "\n",
        "plt.title('Cantidad de Productos con 1 Estrella')\n",
        "plt.xlabel('1 Estrella')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "for p in g1.patches:\n",
        "    g1.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "\n",
        "# 2 ESTRELLAS #######################################\n",
        "plt.figure(figsize=(11,14))\n",
        "plt.subplot(5, 1, 2)\n",
        "\n",
        "g2 = sns.countplot(data = data_grap[data_grap['stars']==2], x='product_category', \n",
        "                   order = data_grap[data_grap['stars']==2]['product_category'].value_counts().index.drop('OTROS').insert(N_TOP+1, 'OTROS'), \n",
        "                   orient=\"v\", palette ='Set3')\n",
        "plt.setp(g2.get_xticklabels(), rotation=90)\n",
        "\n",
        "plt.title('Cantidad de Productos con 2 Estrellas')\n",
        "plt.xlabel('2 Estrellas')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "for p in g2.patches:\n",
        "    g2.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "    \n",
        "# 3 ESTRELLAS #######################################\n",
        "plt.figure(figsize=(11,14))\n",
        "plt.subplot(5, 1, 3)\n",
        "\n",
        "g3 = sns.countplot(data = data_grap[data_grap['stars']==3], x='product_category', \n",
        "                   order = data_grap[data_grap['stars']==3]['product_category'].value_counts().index.drop('OTROS').insert(N_TOP+1, 'OTROS'), \n",
        "                   orient=\"v\", palette ='Set3')\n",
        "plt.setp(g3.get_xticklabels(), rotation=90)\n",
        "\n",
        "plt.title('Cantidad de Productos con 3 Estrellas')\n",
        "plt.xlabel('3 Estrellas')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "for p in g3.patches:\n",
        "    g3.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "    \n",
        "    \n",
        "# 4 ESTRELLAS #######################################\n",
        "plt.figure(figsize=(11,14))\n",
        "plt.subplot(5, 1, 4)\n",
        "\n",
        "g4 = sns.countplot(data = data_grap[data_grap['stars']==4], x='product_category', \n",
        "                   order = data_grap[data_grap['stars']==4]['product_category'].value_counts().index.drop('OTROS').insert(N_TOP+1, 'OTROS'), \n",
        "                   orient=\"v\", palette ='Set3')\n",
        "plt.setp(g4.get_xticklabels(), rotation=90)\n",
        "\n",
        "plt.title('Cantidad de Productos con 4 Estrellas')\n",
        "plt.xlabel('4 Estrellas')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "for p in g4.patches:\n",
        "    g4.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "\n",
        "# 5 ESTRELLAS #######################################\n",
        "plt.figure(figsize=(11,14))\n",
        "plt.subplot(5, 1, 5)\n",
        "\n",
        "g5 = sns.countplot(data = data_grap[data_grap['stars']==5], x='product_category', \n",
        "                   order = data_grap[data_grap['stars']==5]['product_category'].value_counts().index.drop('OTROS').insert(N_TOP+1, 'OTROS'), \n",
        "                   orient=\"v\", palette ='Set3')\n",
        "plt.setp(g5.get_xticklabels(), rotation=90)\n",
        "\n",
        "plt.title('Cantidad de Productos con 5 Estrellas')\n",
        "plt.xlabel('5 Estrellas')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "for p in g5.patches:\n",
        "    g5.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "    \n",
        "#plt.savefig('snscounter.pdf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX9BmmPmzX_k"
      },
      "source": [
        "Los gráicos nos muestran que los tipos de productos se distribuyen casi uniformemente en cada una de las clases a predecir, presentando en todos los casos las mismas 10 principales categorias por estrella.\n",
        "\n",
        "* Home\n",
        "* Wireless\n",
        "* Toy\n",
        "* Sports\n",
        "* PC\n",
        "* Home Improvement\n",
        "* Electronics\n",
        "* Book\n",
        "* Beauty\n",
        "* Kitchen\n",
        "\n",
        "**CONSIDERACIONES**\n",
        "\n",
        "De nuestro dataset original, únicamente utilizaremos las variables de review_body, review_title y star ya que son las únicas que nos aportan información para la predicción.\n",
        "\n",
        "Originalmente se consideró que la variable product_category podría brindarnos información para nuestro problema, al comprobar que la cantidad de instancias por tipos de productos está uniformemente distribuido en cada una de las estrellas, entendemos que no podremos extraer un diferencial de dicha variable. Posiblemente esto suceda porque estamos utilizando un dataset para fines académicos cuyas muestras fueron seleccionadas con esta distribución intencionalmente.\n",
        "\n",
        "Finalmente, el tipo de problema que estamos abordando no requiere de las transformaciones clásicas de datos tales como:\n",
        "\n",
        "* Detección y eliminación de outliers\n",
        "* Encoding\n",
        "* Imnputación de valores faltantes\n",
        "* Escalado de datos\n",
        "* Generación de nuevas variables predictoras/reducción de dimensionalidad (SVD/PCA).\n",
        "\n",
        "Sin embargo, en un apartado posterior se realizarán las transformaciones encesarias para convertir nuestra variable alfanumérica en numérica y así poder ser consumible por los modelos de machine learning a aplicar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9FMfmBZeSUF"
      },
      "source": [
        "Nuevas Variables:\n",
        "A continuación se crearán las siguientes variables:\n",
        "\n",
        "* star_calif: La cuál tomará valor de 0 (Negativo) si la variable star es menor o igual a 3 estrellas y 1 (Positivo) si es mayor.\n",
        "* review_all: La misma concatenará la información provistar por las variables * review_title y review_body"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sQ5HaQIzaHT"
      },
      "source": [
        "# Creamos la variable stars_calif para análisis de sentimiento\n",
        "data['stars_calif'] = [1 if data['stars'][i]> 3 else 0 for i in data.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkOTprBezfSB"
      },
      "source": [
        "# Creo la variable 'review_all', que es una concatenación de 'review_title' y 'review_body'\n",
        "data['review_all']=[(str(data['review_title'][i])+\" \"+str(data['review_body'][i])) for i in data.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Kng1Rvzhco"
      },
      "source": [
        "data['review_all'][5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPHjmHfrzkF2"
      },
      "source": [
        "## Submuestreo de Clases:\n",
        "\n",
        "\n",
        "Para todo el trabajo que se presentará a continuación, la capacidad de procesamiento local con la que se cuenta y la provista por Colab, son insuficientes para poder procesar el dataset de train completo con sus 200 mil instancias. Cabe aclarar al respecto, que se han agotado los intentos por procesar el dataset completo y en todos los casos por tiempos de ejecución o límite de la memoria ram, colab ha interrumpido el trabajo e iniciado a cero la ejecución.\n",
        "\n",
        "Es por lo expuesto que se ha decido realizar un submuestreo incial de 5 mil instancias por clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgcXiiNBznd9"
      },
      "source": [
        "# Submuestro y balanceo de clases\n",
        "data_sample = data.groupby('stars')\n",
        "data_sample = pd.DataFrame(data_sample.apply(lambda x: x.sample(data_sample.size().min()-35000).reset_index(drop=True))).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVsDM9TXz95T"
      },
      "source": [
        "data_train = data_sample.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1kek4vxz_lR"
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0VA-4d70Bja"
      },
      "source": [
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jk997vF0DVC"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(5, 5))\n",
        "\n",
        "# Grafico\n",
        "\n",
        "ax = sns.countplot(data = data_train, x = 'stars', orient=\"v\", palette ='Set3')\n",
        "\n",
        "plt.title('Cantidad Reviews por Stars')\n",
        "plt.xlabel('Star')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UrlT4VA0Iqa"
      },
      "source": [
        "## Analisis de Sentimiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGE0la6w0Hwh"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(5, 5))\n",
        "\n",
        "# Grafico\n",
        "\n",
        "ax = sns.countplot(data = data_train, x = 'stars_calif', orient=\"v\", palette ='Set3')\n",
        "\n",
        "plt.title('Cantidad Reviews por Sentimiento')\n",
        "plt.xlabel('Star')\n",
        "plt.ylabel('Cantidad')\n",
        "\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate('{:,.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCsrNexJ0S15"
      },
      "source": [
        "## Normalizacion "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuqhX2gB0RWj"
      },
      "source": [
        "!python -m spacy download es_core_news_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyJEHD6Q0PXn"
      },
      "source": [
        "import spacy\n",
        "#import es_core_news_sm\n",
        "import es_core_news_md\n",
        "from spacy.lang.es.stop_words import STOP_WORDS\n",
        "\n",
        "import nltk\n",
        "from nltk import Tree\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "#from nltk.corpus import stopwords\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import string\n",
        "from collections import Counter\n",
        "import unicodedata\n",
        "import re\n",
        "import itertools\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nlp = es_core_news_md.load()\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list=  nltk.corpus.stopwords.words('spanish')\n",
        "stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
        "punct = string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eajzCU_00qGF"
      },
      "source": [
        "NLP_train = data_sample.copy()\n",
        "\n",
        "NLP_train_some = NLP_train[['review_all']][0:10].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KtnlXEn0sUL"
      },
      "source": [
        "docex=nlp(NLP_train['review_all'][3])\n",
        "\n",
        "# print column headers\n",
        "print('{:15} | {:15} | {:8} | {:20} | {:11} | {:8} | {:8} | {:8} | '.format(\n",
        "    'TEXT','LEMMA_','POS_','HEAD','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
        "\n",
        "# print various SpaCy POS attributes\n",
        "for token in docex:\n",
        "    print('{:15} | {:15} | {:8} | {:20} | {:11} | {:8} | {:8} | {:8} |'.format(\n",
        "          token.text, token.lemma_, token.pos_, token.head.text, token.dep_\n",
        "        , token.shape_, token.is_alpha, token.is_stop))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzPQ8-uZ0vts"
      },
      "source": [
        "# Similaridad de Coseno entre reviews\n",
        "docex_1=nlp(NLP_train['review_all'][0])\n",
        "docex_2=nlp(NLP_train['review_all'][1])\n",
        "\n",
        "print(docex_1)\n",
        "print(docex_2)\n",
        "print(docex_1.similarity(docex_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToWazTo20yTC"
      },
      "source": [
        "# Similaridad de Coseno entre tokens\n",
        "token1 = docex_1[0]\n",
        "token2 = docex_2[0]\n",
        "\n",
        "print(token1)\n",
        "print(token2)\n",
        "print(token1.similarity(token2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dApI1zhF01NW"
      },
      "source": [
        "# Gráfico de dependencias de una reviw\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in docex.sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVzqzlYk06Q4"
      },
      "source": [
        "# Vector con Word2Vec\n",
        "print(token2.vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcytI6O-1V-v"
      },
      "source": [
        "tokens = []\n",
        "lemma = []\n",
        "pos = []\n",
        "\n",
        "for doc in nlp.pipe(NLP_train_some['review_all'].astype('unicode').values, batch_size=50):\n",
        "    if doc.is_parsed:\n",
        "        # Token\n",
        "        tokens.append([n.text for n in doc])\n",
        "        # Token lematizado y en minúscula\n",
        "        lemma.append([n.lemma_.lower() for n in doc])\n",
        "        # Part of speach\n",
        "        pos.append([n.pos_ for n in doc])\n",
        "    else:\n",
        "        tokens.append(None)\n",
        "        lemma.append(None)\n",
        "        pos.append(None)\n",
        "\n",
        "NLP_train_some['review_tokens'] = tokens\n",
        "NLP_train_some['review_lemma'] = lemma\n",
        "NLP_train_some['review_pos'] = pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVCzuc161YUF"
      },
      "source": [
        "NLP_train_some"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOC0mQWKGum5"
      },
      "source": [
        "**Función de Normalización de texto:**\n",
        "\n",
        "\n",
        "En dicha función, realizaremos el siguiente proceso:\n",
        "\n",
        "Se tokenizarán las reviws. La unidad semántica elegida para la tokenización serán palabras.\n",
        "Se eliminarán las palabras de menos de 2 caracteres.\n",
        "Aplicaremos Lematización, en donde agruparemos a las palabras por su raíz y el rol que cumple en el texto.\n",
        "Eliminaremos las palabras que el lematizador identifique como Pronombre.\n",
        "Llevaremos todas las palabras a minúscula.\n",
        "Eliminaremos las palabras que no transmiten información (Stopwords).\n",
        "Aplicaremos Expresiones Regulares (Regex) para eliminar cualquier patrón en el texto que no nos aporte información."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNUqsXPBG3KJ"
      },
      "source": [
        "stopwords.remove('no')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F44ahCXBG9Yn"
      },
      "source": [
        "def dataCleaning(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if len(token)>1: #si el token tiene más de 1 caracter\n",
        "            # Forma base del token, sin sufijos de flexión. Y lo pasamos a minuscula.\n",
        "            if token.lemma_ != '-PRON-':\n",
        "              temp = token.lemma_.lower()\n",
        "              tokens.append(temp)\n",
        "              clean_tokens = []\n",
        "              # Quitamos stopswords\n",
        "              for token in tokens:\n",
        "                  #if token not in punct and token not in stopwords:\n",
        "                  if token not in stopwords:\n",
        "                      clean_tokens.append(token)\n",
        "    return clean_tokens\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    # Removemos los caracteres especiales\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    \n",
        "    # Eliminamos cualquier caracter que no sen los siguientes: a-z A-Z 0-9   \n",
        "    pattern = r'[^a-zA-Z0-9\\s]' \n",
        "    text = re.sub(pattern, '', text)\n",
        "     \n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hGAYZ2UHAxz"
      },
      "source": [
        "Aplicamos nuestra función de normalización al texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88V8GqfvHebU"
      },
      "source": [
        "titular_list_clean=[]\n",
        "\n",
        "i=0\n",
        "titular_clean=[]\n",
        "for titular in NLP_train['review_all']:\n",
        "    titular=remove_accented_chars(str(titular))\n",
        "    titular_clean=dataCleaning(titular)\n",
        "    titular_list_clean.append(titular_clean)\n",
        "    i=+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2fScG2hHg-8"
      },
      "source": [
        "data_train['Review_Cleaning'] = titular_list_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkmp9RbrH2PO"
      },
      "source": [
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp63ROTfI5tJ"
      },
      "source": [
        "data_review_positivos = data_train[data_train['stars_calif']==1]\n",
        "review_positivos = data_review_positivos['Review_Cleaning'].copy()\n",
        "review_positivos.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SdW_13uJPV8"
      },
      "source": [
        "data_review_negativos = data_train[data_train['stars_calif']==0]\n",
        "review_negativos = data_review_negativos['Review_Cleaning'].copy()\n",
        "review_negativos.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFidiYoJJRwi"
      },
      "source": [
        "todos_titulares_palabras_pos = list(itertools.chain(*review_positivos))\n",
        "todos_titulares_palabras_neg = list(itertools.chain(*review_negativos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky04F0BjJVDb"
      },
      "source": [
        "count_pos=Counter(todos_titulares_palabras_pos)\n",
        "count_neg=Counter(todos_titulares_palabras_neg)\n",
        "\n",
        "bigrams_series_pos = (pd.Series(nltk.ngrams(todos_titulares_palabras_pos, 2)).value_counts())[:20]\n",
        "trigrams_series_pos = (pd.Series(nltk.ngrams(todos_titulares_palabras_pos, 3)).value_counts())[:20]\n",
        "bigrams_series_neg = (pd.Series(nltk.ngrams(todos_titulares_palabras_neg, 2)).value_counts())[:20]\n",
        "trigrams_series_neg = (pd.Series(nltk.ngrams(todos_titulares_palabras_neg, 3)).value_counts())[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sFsqB2zJZlF"
      },
      "source": [
        "# Plot top 50 most frequently\n",
        "common_words = [word[0] for word in count_pos.most_common(50)]\n",
        "common_counts = [word[1] for word in count_pos.most_common(50)]\n",
        "#plt.style.use('dark_background')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "sns.barplot(x=common_words, y=common_counts, ax=ax)\n",
        "plt.title('Top Palabras Frecuentes Reviews Positivos')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfQvm_ImJsl_"
      },
      "source": [
        "bigrams_series_pos.sort_values().plot.barh(width=.9, colormap='Paired', figsize=(12, 8))\n",
        "plt.title('Top Bigramas Frecuentes Reviews Positivos')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tOdQJSUKT-C"
      },
      "source": [
        "trigrams_series_pos.sort_values().plot.barh(width=.9, colormap='Paired',  figsize=(12, 8))\n",
        "plt.title('Top Trigramas Frecuentes Reviews Positivos')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALkhyACFKbNx"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(15, 15))\n",
        "\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(str(common_words))\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title('Word Cloud Reviews Positivos')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPEYlxz4KzmG"
      },
      "source": [
        "# Plot top 50 most frequently\n",
        "common_words = [word[0] for word in count_neg.most_common(50)]\n",
        "common_counts = [word[1] for word in count_neg.most_common(50)]\n",
        "#plt.style.use('dark_background')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "sns.barplot(x=common_words, y=common_counts, ax=ax)\n",
        "plt.title('Top Palabras Frecuentes Reviews Negativos')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AphdS6HFK3ap"
      },
      "source": [
        "bigrams_series_neg.sort_values().plot.barh(width=.9, colormap='Paired', figsize=(12, 8))\n",
        "plt.title('Top Bigramas Frecuentes Reviews Negativos')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9EUna79K7Mc"
      },
      "source": [
        "trigrams_series_neg.sort_values().plot.barh(width=.9, colormap='Paired', figsize=(12, 8))\n",
        "plt.title('Top Bigramas Frecuentes Reviews Negativos')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cvrTjacLINP"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(15, 15))\n",
        "\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(str(common_words))\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title('Word Cloud Reviews Negativos')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu4MJhv_LyaJ"
      },
      "source": [
        "## 3. Transformación de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCM8DE4kLwwC"
      },
      "source": [
        "\n",
        "Seguida de la Normalización de nuestro texto, debemos Vectorizarlo. Es decir, reemplazar a cada instancia por un vector de números que represente a cada uno de los tokens de dicha instancia.\n",
        "\n",
        "En este apartado trabajaremos con dos tipos de vectorizaciones:\n",
        "\n",
        "*** Count Vectorizer:** Convierte la columna de texto en una matriz en la que cada palabra es una columna cuyo valor es el número de veces que dicha palabra aparece en cada review.\n",
        "\n",
        "*** TF-IDF:** Term Frequency-Inverse Document Frequency: Busca puntuaciones de frecuencia de palabras que tratan de resaltar las palabras que son más interesantes, por ejemplo, frecuentes en un documento pero no en todos los documentos. El TfidfVectorizer tokenizará documentos, aprenderá el vocabulario y las ponderaciones inversas de frecuencia de documentos.\n",
        "\n",
        "\n",
        "A continuación, probaremos optimizando el rendimiento de un posterior modelo modificando los valores de los parámetros de countvectorizer y tfidfvectorizer respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k92xbD0MLv72"
      },
      "source": [
        "titular_list_clean=[]\n",
        "\n",
        "i=0\n",
        "titular_clean=[]\n",
        "for titular in data_train['review_all']:\n",
        "    titular=remove_accented_chars(str(titular))\n",
        "    titular_clean=dataCleaning(titular)\n",
        "    titular_clean=' '.join(titular_clean)\n",
        "    titular_list_clean.append(titular_clean)\n",
        "    i=+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2UvHMNQNV6T"
      },
      "source": [
        "result = pd.Series(titular_list_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VgjrlVxNXuj"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAV-fTONORRp"
      },
      "source": [
        "len(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vELpPNJ-OnAV"
      },
      "source": [
        "result.to_csv('result.csv', header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8BqveAdOpNW"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzBQlEjjOsN5"
      },
      "source": [
        "Parámetros de Count Vectorizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNVc78VSO0HK"
      },
      "source": [
        "max_features = [100,200,300,500,700,1000,1300,1500] #No se prueban más features porque agota toda la memoria ram de colab\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "training_auc = [] \n",
        "testing_auc = []\n",
        "\n",
        "\n",
        "for max_feature in max_features:\n",
        "    # Vectorizo con count vectorizer\n",
        "    cvectorizer = CountVectorizer(lowercase=True, strip_accents='unicode', decode_error='ignore', max_features=max_feature)\n",
        "    matriz_titulos_count_vectorizer = cvectorizer.fit_transform(result)\n",
        "\n",
        "    X = matriz_titulos_count_vectorizer.toarray()\n",
        "    Y = data_train['stars']\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=42,stratify=Y)\n",
        "\n",
        "    # Definir el modelo y entreno\n",
        "    clf = LGBMClassifier().fit(X_train,Y_train)\n",
        "    \n",
        "    # Binarize ytest with shape (n_samples, n_classes)\n",
        "    Y_test = label_binarize(Y_test, classes=labels)\n",
        "\n",
        "    # Predecir y evaluar sobre el set de entrenamiento\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_train_preds = label_binarize(y_train_pred, classes=labels)\n",
        "    train_roc_auc_score = roc_auc_score(Y_train,y_train_preds, multi_class='ovr')\n",
        "    \n",
        "    \n",
        "    # Predecir y evaluar sobre el set de evaluación\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    y_test_preds = label_binarize(y_test_pred, classes=labels)\n",
        "    test_roc_auc_score = roc_auc_score(Y_test,y_test_preds, multi_class='ovr') \n",
        "    \n",
        "    # Agregar la información a las listas\n",
        "    training_auc.append(train_roc_auc_score)\n",
        "    testing_auc.append(test_roc_auc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK_7BJZEO6yX"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(6, 6))\n",
        "plt.plot(max_features, training_auc, color='blue', label='Training Roc AUC Score')\n",
        "plt.plot(max_features, testing_auc, color='green', label='Testing Roc AUC Score')\n",
        "plt.xlabel('Max Features')\n",
        "plt.ylabel('Roc AUC Score')\n",
        "plt.title('Hyperparameter Tuning', pad=15, size=15)\n",
        "plt.legend()\n",
        "#plt.savefig('error.png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1m_UFd2O-ur"
      },
      "source": [
        "# Submuestro y balanceo de clases\n",
        "data_sample_min = data_train.groupby('stars')\n",
        "data_sample_min = pd.DataFrame(data_sample_min.apply(lambda x: x.sample(data_sample_min.size().min()-4500).reset_index(drop=True))).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT56PAxmPAoT"
      },
      "source": [
        "data_sample_min.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ysbBLIvPC0X"
      },
      "source": [
        "data_sample_min.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLlm3_ifPZbJ"
      },
      "source": [
        "titular_list_clean=[]\n",
        "\n",
        "i=0\n",
        "titular_clean=[]\n",
        "for titular in data_sample_min['review_all']:\n",
        "    titular=remove_accented_chars(str(titular))\n",
        "    titular_clean=dataCleaning(titular)\n",
        "    titular_clean=' '.join(titular_clean)\n",
        "    titular_list_clean.append(titular_clean)\n",
        "    i=+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K1LbthvPdBi"
      },
      "source": [
        "result_data_sample_min = pd.Series(titular_list_clean)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqQxq0xCP9dR"
      },
      "source": [
        "result_data_sample_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ2HM3EKQAgM"
      },
      "source": [
        "len(result_data_sample_min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ednTOY3WQCcp"
      },
      "source": [
        "type(result_data_sample_min)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thAO1cnYQrb5"
      },
      "source": [
        "# Al construir el vocabulario, ignore los términos que tienen una frecuencia de documento estrictamente más alta que el umbral dado\n",
        "\n",
        "max_dfs  = [0.8,0.9,1]\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "training_auc = [] \n",
        "testing_auc = []\n",
        "\n",
        "\n",
        "for max_df in max_dfs:\n",
        "    # Vectorizo con count vectorizer\n",
        "    cvectorizer = CountVectorizer(lowercase=True, strip_accents='unicode', decode_error='ignore', max_df=max_df)\n",
        "    matriz_titulos_count_vectorizer = cvectorizer.fit_transform(result_data_sample_min)\n",
        "\n",
        "    X = matriz_titulos_count_vectorizer.toarray()\n",
        "    Y = data_sample_min['stars']\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=42,stratify=Y)\n",
        "\n",
        "    # Definir el modelo y entreno\n",
        "    clf = LGBMClassifier().fit(X_train,Y_train)\n",
        "    \n",
        "    # Binarize ytest with shape (n_samples, n_classes)\n",
        "    Y_test = label_binarize(Y_test, classes=labels)\n",
        "\n",
        "    # Predecir y evaluar sobre el set de entrenamiento\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_train_preds = label_binarize(y_train_pred, classes=labels)\n",
        "    train_roc_auc_score = roc_auc_score(Y_train,y_train_preds, multi_class='ovr')\n",
        "    \n",
        "    \n",
        "    # Predecir y evaluar sobre el set de evaluación\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    y_test_preds = label_binarize(y_test_pred, classes=labels)\n",
        "    test_roc_auc_score = roc_auc_score(Y_test,y_test_preds, multi_class='ovr') \n",
        "    \n",
        "    # Agregar la información a las listas\n",
        "    training_auc.append(train_roc_auc_score)\n",
        "    testing_auc.append(test_roc_auc_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1f0xP6cX2DF"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(6, 6))\n",
        "plt.plot(max_dfs, training_auc, color='blue', label='Training Roc AUC Score')\n",
        "plt.plot(max_dfs, testing_auc, color='green', label='Testing Roc AUC Score')\n",
        "plt.xlabel('Max Df')\n",
        "plt.ylabel('Roc AUC Score')\n",
        "plt.title('Hyperparameter Tuning', pad=15, size=15)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5xXYA8zX5F0"
      },
      "source": [
        "Parámetros de TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOvgG14BX9U4"
      },
      "source": [
        "max_features = [100,200,300,500,700,1000,1300,1500]\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "training_auc = [] \n",
        "testing_auc = []\n",
        "\n",
        "\n",
        "for max_feature in max_features:\n",
        "    # Vectorizo con count vectorizer\n",
        "    tvectorizer = TfidfVectorizer(lowercase=True, strip_accents='unicode', decode_error='ignore', max_features=max_feature)\n",
        "    matriz_titulos_count_vectorizer = tvectorizer.fit_transform(result)\n",
        "\n",
        "    X = matriz_titulos_count_vectorizer.toarray()\n",
        "    Y = data_train['stars']\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=42,stratify=Y)\n",
        "\n",
        "    # Definir el modelo y entreno\n",
        "    clf = LGBMClassifier().fit(X_train,Y_train)\n",
        "    \n",
        "    # Binarize ytest with shape (n_samples, n_classes)\n",
        "    Y_test = label_binarize(Y_test, classes=labels)\n",
        "\n",
        "    # Predecir y evaluar sobre el set de entrenamiento\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_train_preds = label_binarize(y_train_pred, classes=labels)\n",
        "    train_roc_auc_score = roc_auc_score(Y_train,y_train_preds, multi_class='ovr')\n",
        "    \n",
        "    \n",
        "    # Predecir y evaluar sobre el set de evaluación\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    y_test_preds = label_binarize(y_test_pred, classes=labels)\n",
        "    test_roc_auc_score = roc_auc_score(Y_test,y_test_preds, multi_class='ovr') \n",
        "    \n",
        "    # Agregar la información a las listas\n",
        "    training_auc.append(train_roc_auc_score)\n",
        "    testing_auc.append(test_roc_auc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16sEj_ywY6RB"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(6, 6))\n",
        "plt.plot(max_features, training_auc, color='blue', label='Training Roc AUC Score')\n",
        "plt.plot(max_features, testing_auc, color='green', label='Testing Roc AUC Score')\n",
        "plt.xlabel('Max Features')\n",
        "plt.ylabel('Roc AUC Score')\n",
        "plt.title('Hyperparameter Tuning', pad=15, size=15)\n",
        "plt.legend()\n",
        "#plt.savefig('error.png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTKymZCaZBqQ"
      },
      "source": [
        "# Al construir el vocabulario, ignore los términos que tienen una frecuencia de documento estrictamente más alta que el umbral dado\n",
        "\n",
        "max_dfs  = [0.7,0.8,0.9,1]\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "training_auc = [] \n",
        "testing_auc = []\n",
        "\n",
        "\n",
        "for max_df in max_dfs:\n",
        "    # Vectorizo con count vectorizer\n",
        "    tvectorizer = TfidfVectorizer(lowercase=True, strip_accents='unicode', decode_error='ignore', max_df=max_df)\n",
        "    matriz_titulos_count_vectorizer = tvectorizer.fit_transform(result_data_sample_min)\n",
        "\n",
        "    X = matriz_titulos_count_vectorizer.toarray()\n",
        "    Y = data_sample_min['stars']\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=42,stratify=Y)\n",
        "\n",
        "    # Definir el modelo y entreno\n",
        "    clf = LGBMClassifier().fit(X_train,Y_train)\n",
        "    \n",
        "    # Binarize ytest with shape (n_samples, n_classes)\n",
        "    Y_test = label_binarize(Y_test, classes=labels)\n",
        "\n",
        "    # Predecir y evaluar sobre el set de entrenamiento\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_train_preds = label_binarize(y_train_pred, classes=labels)\n",
        "    train_roc_auc_score = roc_auc_score(Y_train,y_train_preds, multi_class='ovr')\n",
        "    \n",
        "    \n",
        "    # Predecir y evaluar sobre el set de evaluación\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "    y_test_preds = label_binarize(y_test_pred, classes=labels)\n",
        "    test_roc_auc_score = roc_auc_score(Y_test,y_test_preds, multi_class='ovr') \n",
        "    \n",
        "    # Agregar la información a las listas\n",
        "    training_auc.append(train_roc_auc_score)\n",
        "    testing_auc.append(test_roc_auc_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9kt1I2_ZMgn"
      },
      "source": [
        "plt.rc(\"figure\", figsize=(6, 6))\n",
        "plt.plot(max_dfs, training_auc, color='blue', label='Training Roc AUC Score')\n",
        "plt.plot(max_dfs, testing_auc, color='green', label='Testing Roc AUC Score')\n",
        "plt.xlabel('Max Df')\n",
        "plt.ylabel('Roc AUC Score')\n",
        "plt.title('Hyperparameter Tuning', pad=15, size=15)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElGIZ05wZRQz"
      },
      "source": [
        "VECTORIZACIÓN FINAL\n",
        "\n",
        "En ambos casos, los parámetros que mejorarían nuestra performance del modelo son similares, motivo por el cual elegiremos los mismos para ambos tipos de vectorización."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0slRkYcCZgKp"
      },
      "source": [
        "# numero minimo y maximo de tokens consecutivos que se consideran\n",
        "MIN_NGRAMS=1\n",
        "MAX_NGRAMS=4\n",
        "# cantidad maxima de docs que tienen que tener a un token para conservarlo.\n",
        "MAX_DF= 0.8\n",
        "max_features=1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJK241raau2Y"
      },
      "source": [
        "cvectorizer = CountVectorizer(lowercase=True, strip_accents='unicode', decode_error='ignore',\n",
        "                             ngram_range=(MIN_NGRAMS, MAX_NGRAMS), max_df=MAX_DF, max_features=max_features)\n",
        "matriz_titulos_count_vectorizer = cvectorizer.fit_transform(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtlfywDsaxAk"
      },
      "source": [
        "tvectorizer = TfidfVectorizer(lowercase=True, strip_accents='unicode', decode_error='ignore',\n",
        "                             ngram_range=(MIN_NGRAMS, MAX_NGRAMS), max_df=MAX_DF, max_features=max_features)\n",
        "matriz_titulos_count_tvectorizer = tvectorizer.fit_transform(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAZaW_qebA0o"
      },
      "source": [
        "## 4. Modelos de Machine Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnhnKuFKbG6H"
      },
      "source": [
        "En este apartado nos propondremos contruir un clasificador que prediga la cantidad de estrellas con la que puntuará un cliente una compra a partir de su reseña. Para ello, trabajaremos con las siguientes iteraciones:\n",
        "\n",
        "**Iteración 1:** Se entrenará un modelo LGBM Classifier con CountVectorizer y TfidfVectorizer para probar el desempeño de ambos vectorizers y elegir el que mejor performe. Asimimo, evaluaremos la importancia de atributos y observaremos gráficamente las reglas de decisión principales. El modelo resultante de este apartado será nuestro benchmark.\n",
        "\n",
        "**Iteración 2:** Se seleccionarán las características principales de nuestro total de features lo cual nos permitirá entrenar el modelo con más instancias. Asimismo, ee entrenarán diferentes modelos para evaluar si es necesario aplicar una técnica de boosting o podemos obtener los mismos resultados con un modelo más simple.\n",
        "\n",
        "**Iteración 3:** Se seleccionará el mejor modelo y se aplicará Randomized Search para optimizarán hiperparámtros y evaluar la estabilidad del modelo con 5 k fold validation.\n",
        "\n",
        "Como consideración, para evaluar el rendimiento del modelo multiclase, se utilizará la métrica con ROC Curve, la cual calcula el área bajo la curva del receptor (ROC AUC) a partir de los resultados de las predicciones, cuyo resultado será el valor promedio del score obtenido en cada una de las clases contra todas las demás."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1ª Iteración "
      ],
      "metadata": {
        "id": "kqiyRWwmB4iT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWSU4tbbPR0"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import graphviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuo3N7xQbRvJ"
      },
      "source": [
        "# COUNT VECTORIZER\n",
        "X1 = matriz_titulos_count_vectorizer.toarray()\n",
        "Y1 = data_train['stars']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1,Y1,test_size=0.3,random_state=42,stratify=Y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yv9Y76lbUNZ"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_la0-8JtbWhJ"
      },
      "source": [
        "LGBM_cl= LGBMClassifier().fit(X_train,y_train)\n",
        "\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "# Binarize ytest with shape (n_samples, n_classes)\n",
        "y_testb = label_binarize(y_test, classes=labels)\n",
        "\n",
        "y_train_pred = LGBM_cl.predict(X_train)\n",
        "y_test_pred = LGBM_cl.predict(X_test)\n",
        "\n",
        "# Binarize ypreds with shape (n_samples, n_classes)\n",
        "y_train_preds = label_binarize(y_train_pred, classes=labels)\n",
        "y_test_preds = label_binarize(y_test_pred, classes=labels)\n",
        "    \n",
        "print('Modelo: LGBM_cl')\n",
        "print('ROC AUC Train', roc_auc_score(y_train,y_train_preds, multi_class='ovr'))\n",
        "print('ROC AUC Test', roc_auc_score(y_test,y_test_preds, multi_class='ovr'))\n",
        "metrics.plot_confusion_matrix(LGBM_cl, X_test, y_test, values_format = '.0f')\n",
        "plt.show()\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3AYvyOibg7k"
      },
      "source": [
        "# TF-IDF VECTORIZER\n",
        "X2 = matriz_titulos_count_tvectorizer.toarray()\n",
        "Y2 = data_train['stars']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2,Y2,test_size=0.3,random_state=42,stratify=Y2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXQYDumMbihR"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Yw9WsQblT1"
      },
      "source": [
        "LGBM_cl= LGBMClassifier().fit(X_train,y_train)\n",
        "\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "# Binarize ytest with shape (n_samples, n_classes)\n",
        "y_testb = label_binarize(y_test, classes=labels)\n",
        "\n",
        "y_train_pred = LGBM_cl.predict(X_train)\n",
        "y_test_pred = LGBM_cl.predict(X_test)\n",
        "\n",
        "# Binarize ypreds with shape (n_samples, n_classes)\n",
        "y_train_preds = label_binarize(y_train_pred, classes=labels)\n",
        "y_test_preds = label_binarize(y_test_pred, classes=labels)\n",
        "    \n",
        "print('Modelo: LGBM_cl')\n",
        "print('ROC AUC Train', roc_auc_score(y_train,y_train_preds, multi_class='ovr'))\n",
        "print('ROC AUC Test', roc_auc_score(y_test,y_test_preds, multi_class='ovr'))\n",
        "metrics.plot_confusion_matrix(LGBM_cl, X_test, y_test, values_format = '.0f')\n",
        "plt.show()\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qstqsqD_btHF"
      },
      "source": [
        "Se puede observar una pequeña mejoría aplicando tf-idf, por lo que se seleccionará dicho modelo como nuestro benchmark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj0KH4CBbv10"
      },
      "source": [
        "# Ordeno las features más importantes\n",
        "thresholds = sorted(LGBM_cl.feature_importances_, reverse=True)\n",
        "\n",
        "# Me quedo con las primeras 100\n",
        "thresholds = thresholds[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq6iT16Ubzyo"
      },
      "source": [
        "# Evalúo el ROC AUC que obtengo incorporando de a una feature al modelo\n",
        "for thresh in thresholds:\n",
        "    # select features using threshold\n",
        "    selection = SelectFromModel(LGBM_cl, threshold=thresh, prefit=True)\n",
        "    select_X_train = selection.transform(X_train)\n",
        "    \n",
        "    # train model\n",
        "    selection_model = LGBMClassifier(random_state=7)\n",
        "    selection_model.fit(select_X_train, y_train)\n",
        "    \n",
        "    # eval model\n",
        "    select_X_test = selection.transform(X_test)\n",
        "    y_pred = selection_model.predict(select_X_test)\n",
        "    y_preds = label_binarize(y_pred, classes=labels)\n",
        "    predictions = [np.round(value) for value in y_preds]\n",
        "    roc_auc = roc_auc_score(y_test,predictions, multi_class='ovr')\n",
        "    print(\"Thresh=%.3f, n=%d, ROC AUC: %.2f%%\" % (thresh, select_X_train.shape[1], roc_auc*100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rgkV6tHcJgZ"
      },
      "source": [
        "# Genero un dataframe con las features y su importancia\n",
        "\n",
        "atributos = tvectorizer.get_feature_names()\n",
        "feat_imp = pd.DataFrame({'Atributo':atributos,'importancia':LGBM_cl.feature_importances_}).sort_values('importancia',ascending=False)\n",
        "most_important_features = feat_imp[:200]\n",
        "\n",
        "# Exporto para poder analizar\n",
        "#feat_imp.to_excel('feature_importance.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4kBM2URcMcj"
      },
      "source": [
        "feat_imp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWCMgnwacPuj"
      },
      "source": [
        "N = 30\n",
        "first_N = feat_imp[:N]\n",
        "\n",
        "first_N[:30].sort_values('importancia',ascending=True).plot.barh(x='Atributo', width=.9, colormap='Paired', figsize=(12, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlzwdUKqdEoa"
      },
      "source": [
        "# print column headers\n",
        "print('{:15} | {:15} | {:8} | {:20} | {:11} | {:8} | {:8} | {:8} | '.format('TEXT','LEMMA_','POS_','HEAD','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
        "\n",
        "for i in first_N['Atributo']:\n",
        "  doc=nlp(i)\n",
        "  # print various SpaCy POS attributes\n",
        "  for token in doc:\n",
        "    print('{:15} | {:15} | {:8} | {:20} | {:11} | {:8} | {:8} | {:8} |'.format(token.text, token.lemma_, token.pos_, token.head.text, token.dep_ , token.shape_, token.is_alpha, token.is_stop))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM_-andzdojO"
      },
      "source": [
        "DecisionTree_cl = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5).fit(X_train,y_train)\n",
        "\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "# Binarize ytest with shape (n_samples, n_classes)\n",
        "y_testb = label_binarize(y_test, classes=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wykmBYUBdquv"
      },
      "source": [
        "dot_data = tree.export_graphviz(DecisionTree_cl, out_file=None, feature_names=tvectorizer.get_feature_names(),rotate = True, filled=True)\n",
        "\n",
        "# Draw graph\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaTrZWBPd7vZ"
      },
      "source": [
        "### **4.2 Iteración 2**\n",
        "\n",
        "\n",
        "En la siguiente iteración, entrenaremos los siguientes modelos con las características principales seleccionadas:\n",
        "\n",
        "- Gaussian Classifier\n",
        "- K Neighbors Classifier\n",
        "- Decision Tree Classifier\n",
        "- Random Forest Classifier\n",
        "- XGB Classifier\n",
        "- LGBM Classifier\n",
        "\n",
        "\n",
        "Asimismo, redefiniremos la función de normalización de texto quedándonos sólo con los tokens cuyo part of speach está dentro de las 30 características más importantes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGieWbsjd7L7"
      },
      "source": [
        "# Submuestro y balanceo de clases\n",
        "data_sample = data.groupby('stars')\n",
        "data_sample = pd.DataFrame(data_sample.apply(lambda x: x.sample(data_sample.size().min()-25000).reset_index(drop=True))).reset_index(drop=True)\n",
        "data_train = data_sample.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0m-Q2UoeGkl"
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOgdXZB3eI-e"
      },
      "source": [
        "def dataCleaning(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if len(token)>1: #si el token tiene más de 1 caracteres\n",
        "            # Forma base del token, sin sufijos de flexión. Y lo pasamos a minuscula.\n",
        "            if token.lemma_ != '-PRON-' and (token.pos_ == 'ADV' or token.pos_ == 'ADJ' or token.pos_ == 'VERB' or token.pos_ == 'PROPN' or token.pos_ == 'NOUN'): \n",
        "              temp = token.lemma_.lower()\n",
        "              tokens.append(temp)\n",
        "              clean_tokens = []\n",
        "              # Quitamos stopswords\n",
        "              for token in tokens:\n",
        "                  #if token not in punct and token not in stopwords:\n",
        "                  if token not in stopwords:\n",
        "                      clean_tokens.append(token)\n",
        "    return clean_tokens\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    # Removemos los caracteres especiales\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    \n",
        "    # Eliminamos cualquier caracter que no sen los siguientes: a-z A-Z 0-9   \n",
        "    pattern = r'[^a-zA-Z0-9\\s]' \n",
        "    text = re.sub(pattern, '', text)\n",
        "     \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSLi1T8JeLRl"
      },
      "source": [
        "titular_list_clean=[]\n",
        "\n",
        "i=0\n",
        "titular_clean=[]\n",
        "for titular in data_train['review_all']:\n",
        "    titular=remove_accented_chars(str(titular))\n",
        "    titular_clean=dataCleaning(titular)\n",
        "    titular_clean=' '.join(titular_clean)\n",
        "    titular_list_clean.append(titular_clean)\n",
        "    i=+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEgdiNmpo-x_"
      },
      "source": [
        "result = pd.Series(titular_list_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl3GfLL6o_hw"
      },
      "source": [
        "len(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPiyqrnupDKk"
      },
      "source": [
        "X_modelo = pd.DataFrame(matriz_titulos_count_tvectorizer.toarray(), columns=tvectorizer.get_feature_names())\n",
        "X_modelo.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ScAwDxYpNpm"
      },
      "source": [
        "Gaussian_cl = GaussianNB().fit(X_train,y_train)\n",
        "KNeighbors_cl = KNeighborsClassifier().fit(X_train,y_train)\n",
        "DecisionTree_cl = DecisionTreeClassifier().fit(X_train,y_train)\n",
        "RandomForest_cl = RandomForestClassifier().fit(X_train,y_train)\n",
        "XGB_cl = XGBClassifier().fit(X_train,y_train)\n",
        "LGBM_cl= LGBMClassifier().fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz9AnTNIpnun"
      },
      "source": [
        "labels = [1, 2, 3, 4, 5]\n",
        "# Binarize ytest with shape (n_samples, n_classes)\n",
        "y_testb = label_binarize(y_test, classes=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yxu6gKArR7I"
      },
      "source": [
        "modelos = ['Gaussian Classifier', 'K Neighbors Classifier', 'Decision Tree Classifier', 'Random Forest Classifier', 'XGB Classifier', 'LGBM Classifier']\n",
        "\n",
        "for i, model in enumerate([Gaussian_cl, KNeighbors_cl, DecisionTree_cl, RandomForest_cl, XGB_cl, LGBM_cl]):\n",
        "    \n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    # Binarize ypreds with shape (n_samples, n_classes)\n",
        "    y_train_preds = label_binarize(y_train_pred, classes=labels)\n",
        "    y_test_preds = label_binarize(y_test_pred, classes=labels)\n",
        "        \n",
        "    print(f'Modelo: {modelos[i]}')\n",
        "    print('ROC AUC Train', roc_auc_score(y_train,y_train_preds, multi_class='ovr'))\n",
        "    print('ROC AUC Test', roc_auc_score(y_test,y_test_preds, multi_class='ovr'))\n",
        "    metrics.plot_confusion_matrix(model, X_test, y_test, values_format = '.0f')\n",
        "    plt.show()\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeJ23j6WrhYZ"
      },
      "source": [
        "De los modelos construidos en la presente iteración, los modelos de árboles de clasificación son lo únicos que han podido predecir mejor las clases extremas (1 estrella y 5 estrella) y presentado un menor error en la predicción de las clases ambiguas (2, 3 y 4 estrellas). Sin embaego, nuestro modelo de benchmark junto con el XGB Classifier son los únicos que no han presentando indicios de overfiting. Por su mejor performance en tiempos de ejecución y en rendimientos de la métrica elegida, es que continuaremos trabajando con el LGBM Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrct9iqDr1Nq"
      },
      "source": [
        "### **4.3 Iteracion 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTIyHr3Vryms"
      },
      "source": [
        "\n",
        "Por los motivos anteriormente enunciadas, no podremos hacer una búsqueda exhautiva de hiperparámetros con técnicas como Grid Search, ya que por la cantidad de features la ejecución se demora más de los límites de tiempos permitidos por Colab.\n",
        "\n",
        "En la presente iteración realizaremos una optimización de hiperparámetros con Random Search aplicado k fold validation, cuya técnica nos permite recorrer una mayor rango de posibilidades a una cantidad de iteracion prefijadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8TUbr_Tr7WB"
      },
      "source": [
        "param_trees = {'n_estimators': [100, 150, 200, 250],  # Número de árboles \n",
        "               'metodo': ['SVD', 'KBEST'],\n",
        "               'max_depth': [10, 15, 20, 50, 60,-1],  # Profundidad\n",
        "               'num_leaves': [7, 14, 21, 30, 50, 60], # Máximo de hojas de árboles\n",
        "               'min_child_samples':[15, 20, 30, -1],  # Número mínimo de datos necesarios en un niño (hoja)\n",
        "               }\n",
        "\n",
        "roc_auc_ovr_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr')\n",
        "\n",
        "# ESTRATEGIA 2: Random Search\n",
        "model = LGBMClassifier(random_state=42, subsample=0.7)\n",
        "rs = RandomizedSearchCV(model, param_trees, n_iter=50, scoring=roc_auc_ovr_scorer, verbose=2 , n_jobs=3)\n",
        "rs.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wGQm6k1sC8-"
      },
      "source": [
        "print(\"Mejores parametros: \"+str(rs.best_params_))\n",
        "print(\"Mejor Score: \"+str(rs.best_score_)+'\\n')\n",
        "\n",
        "scores_2 = pd.DataFrame(rs.cv_results_)\n",
        "scores_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h5EJPqNsMvf"
      },
      "source": [
        "LGBM_clf = LGBMClassifier(n_estimators=250, metodo='KBEST', min_child_samples=30, num_leaves=30, random_state=42)\n",
        "LGBM_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxXT4MdXsh0r"
      },
      "source": [
        "y_train_pred = LGBM_clf.predict(X_train)\n",
        "y_test_pred = LGBM_clf.predict(X_test)\n",
        "\n",
        "# Binarize ypreds with shape (n_samples, n_classes)\n",
        "y_train_preds = label_binarize(y_train_pred, classes=labels)\n",
        "y_test_preds = label_binarize(y_test_pred, classes=labels)\n",
        "        \n",
        "print('ROC AUC Train', roc_auc_score(y_train,y_train_preds, multi_class='ovr'))\n",
        "print('ROC AUC Test', roc_auc_score(y_test,y_test_preds, multi_class='ovr'))\n",
        "metrics.plot_confusion_matrix(LGBM_clf, X_test, y_test, values_format = '.0f')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHKHF9WddjDn"
      },
      "source": [
        "Con nuestra optimización, obtenemos una performace muy parecida a la presentada previamente. Si bien no se ha logrdo mejorar el rendimiento de la clasificación, nos aseguramos de que nuestros resultados son estables y no producto del azar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxiBeoqyrfNG"
      },
      "source": [
        "## Investigacion: \n",
        "\n",
        "En el apartado anterior pudimos observar que en el mejor de los casos, logramos construir un modelo que puede diferenciar una review de 1 estrella a una de 5 estrellas, pero que no logra clasificar adecuadamente aquellas reviews con puntuaciones intermedias de 2, 3 y 4 estrellas.\n",
        "\n",
        "En este punto nos preguntamos: ¿Valdrá la pena convertir el problema de Machine Learning en un problema binario? Es decir, asignar únicamente las etiquetas Positiva y Negativa a cada crítica y hacer un modelo que, en lugar de predecir las estrellas, prediga esa etiqueta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1v0CDEhrq-A"
      },
      "source": [
        "Teniendo en cuenta que una puntuación de 2 a 3 estrellas o de 3 a 4 estrellas puede ser una apreciación muy subjetiva colmada de grises, es que nuestra hipótesis será que un clasificador binario performará mucho mejor en la tarea de predecir si un producto será puntuado como positivo o negativo.\n",
        "\n",
        "Para realizar esta investigación realizaremos el mismo proceso de Normalización y Vectorización de los datos, tomando sólo aquellas reviews que pertenezcan a alguno de los extremos de nuestro problema:\n",
        "\n",
        "Reviews Negativas: 1 y 2 estrellas\n",
        "Reviews Positivas: 4 y 5 estellas.\n",
        "Las instancias pertenecientes a 3 estrellas serán desestimadas por 2 motivos: el primero, por practicidad para el balanceo de clases y el segundo, porque en el hipotético caso en que el usuario tuviese únicamente la opción de calificar como conforme o inconforme, la situación parcial de 3 estrellas queda sin efecto.\n",
        "\n",
        "En el presente aparatado, construiremos los mismos clasificadores que en el segmento anterior y evaluaremos su performance con la métrica de Accuracy, ya que es equivalente a ROC AUC para clases balanceadas. Finalmente, optimizatemos hiperparámetros del modelo que obtenga mejor rendimiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgouX0c1rsEh"
      },
      "source": [
        "# Balanceamos clases y eliminamos el punto medio, cuya existencia no sería tal en un problema de clasificación positiva / negativa:\n",
        "\n",
        "data_train_sent = data_train[data_train['stars']!=3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qChCPhFPtZPp"
      },
      "source": [
        "data_train_sent.stars_calif.value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVOrgyAutc9K"
      },
      "source": [
        "titular_list_clean=[]\n",
        "\n",
        "i=0\n",
        "titular_clean=[]\n",
        "for titular in data_train_sent['review_all']:\n",
        "    titular=remove_accented_chars(str(titular))\n",
        "    titular_clean=dataCleaning(titular)\n",
        "    titular_clean=' '.join(titular_clean)\n",
        "    titular_list_clean.append(titular_clean)\n",
        "    i=+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RcetKm9tfGN"
      },
      "source": [
        "result_sent = pd.Series(titular_list_clean)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY-B_otvtgu0"
      },
      "source": [
        "result_sent\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bo3-BAgtj0p"
      },
      "source": [
        "# numero minimo y maximo de tokens consecutivos que se consideran\n",
        "MIN_NGRAMS=1\n",
        "MAX_NGRAMS=4\n",
        "# cantidad maxima de docs que tienen que tener a un token para conservarlo.\n",
        "MAX_DF= 0.8\n",
        "max_features=1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqbs0bZStmKc"
      },
      "source": [
        "tvectorizer = TfidfVectorizer(lowercase=True, strip_accents='unicode', decode_error='ignore',\n",
        "                             ngram_range=(MIN_NGRAMS, MAX_NGRAMS), max_df=MAX_DF, max_features=max_features)\n",
        "matriz_titulos_tfidf_tvectorizer = tvectorizer.fit_transform(result_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu9NDeV5tocN"
      },
      "source": [
        "X4 = matriz_titulos_tfidf_tvectorizer.toarray()\n",
        "Y4 = data_train_sent['stars_calif']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X4,Y4,test_size=0.3,random_state=42,stratify=Y4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMqhKTB7tqy-"
      },
      "source": [
        "Gaussian_cl = GaussianNB().fit(X_train,y_train)\n",
        "KNeighbors_cl = KNeighborsClassifier().fit(X_train,y_train)\n",
        "DecisionTree_cl = DecisionTreeClassifier().fit(X_train,y_train)\n",
        "RandomForest_cl = RandomForestClassifier().fit(X_train,y_train)\n",
        "XGB_cl = XGBClassifier().fit(X_train,y_train)\n",
        "LGBM_cl= LGBMClassifier().fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxusXpCzttLd"
      },
      "source": [
        "modelos = ['Gaussian Classifier', 'K Neighbors Classifier', 'Decision Tree Classifier', 'Random Forest Classifier', 'XGB Classifier', 'LGBM Classifier']\n",
        "\n",
        "for i, model in enumerate([Gaussian_cl, KNeighbors_cl, DecisionTree_cl, RandomForest_cl, XGB_cl, LGBM_cl]):\n",
        "    \n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "        \n",
        "    print(f'Modelo: {modelos[i]}')\n",
        "    print('Accuracy Train', accuracy_score(y_train,y_train_pred))\n",
        "    print('Accuracy Test', accuracy_score(y_test,y_test_pred))\n",
        "    metrics.plot_confusion_matrix(model, X_test, y_test, values_format = '.0f')\n",
        "    plt.show()\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfhweVoftv91"
      },
      "source": [
        "param_trees = {'n_estimators': [100, 150, 200, 250],  # Número de árboles \n",
        "               'metodo': ['SVD', 'KBEST'],\n",
        "               'max_depth': [10, 15, 20, 50, 60,-1],  # Profundidad\n",
        "               'num_leaves': [7, 14, 21, 30, 50, 60], # Máximo de hojas de árboles\n",
        "               'min_child_samples':[15, 20, 30, -1],  # Número mínimo de datos necesarios en un niño (hoja)\n",
        "               }\n",
        "\n",
        "# ESTRATEGIA 2: Random Search\n",
        "model = LGBMClassifier(random_state=42, subsample=0.7)\n",
        "rs = RandomizedSearchCV(model, param_trees, n_iter=50, scoring= 'accuracy', verbose=2 , n_jobs=3)\n",
        "rs.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgpRFa9TtyOE"
      },
      "source": [
        "print(\"Mejores parametros: \"+str(rs.best_params_))\n",
        "print(\"Mejor Score: \"+str(rs.best_score_)+'\\n')\n",
        "\n",
        "scores_2 = pd.DataFrame(rs.cv_results_)\n",
        "scores_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X-47bzLt0my"
      },
      "source": [
        "LGBM_clf = LGBMClassifier(n_estimators=250, metodo='KBEST', min_child_samples=30, num_leaves=30, random_state=42)\n",
        "LGBM_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2mJXvBLt2ru"
      },
      "source": [
        "y_train_pred = LGBM_clf.predict(X_train)\n",
        "y_test_pred = LGBM_clf.predict(X_test)\n",
        "        \n",
        "print('Accuracy Train', accuracy_score(y_train,y_train_pred))\n",
        "print('Accuracy Test', accuracy_score(y_test,y_test_pred))\n",
        "metrics.plot_confusion_matrix(LGBM_clf, X_test, y_test, values_format = '.0f')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vuf8hL4hiNc"
      },
      "source": [
        "## Investigacion (2): Utilizacion de redes neuronales para analisis de sentimientos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Zp3ETSZAvF"
      },
      "source": [
        "Hasta el momento, hemos realizado análisis de tipo descriptivo, haciendo un mapeo de las reviews buscando obtener un primer vistazo de la muestra, y hemos realizado modelos de machine learning para obtener distintas respuestas de predicción de nuestros datos\n",
        "\n",
        "\n",
        "En este apartado nos planteamos como hipótesis de investigación de trabajo la relevancia de utilizar el análisis de sentimientos y posibles usos de esta técnica para la base de datos de Amazon. Esta tecnica se ha vuelto en una de las predilectas debiido a su amplia versatilidad para buscar obtener la opinion de los distintos usuarios de un determinado servicio mediante relaciones estadísticas y de asociación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QADGSKV1gkhP"
      },
      "source": [
        "Hay muchos enfoques utilizados para realizar este análisis, en dónde las de mayor popularidad abundan en las técnicas de machine learning como lo son el modelo \"Multinomial de Bayes\" o de maquinas de soporte vectorial. Sin embargo, de acuerdo a cierta literatura académica, los modelos de Deep Learning son los de mejor performance y precisión; como sostienen Liao (2017), Zhang (2018) y Liu (2015). Esto se puede revisar en los links adjuntos más abajo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t7Bk48IjHNr"
      },
      "source": [
        "Links a los trabajos citados:\n",
        "* Liao (2017): https://www.sciencedirect.com/science/article/pii/S1877050917312103\n",
        "* Zhang (2018): https://arxiv.org/ftp/arxiv/papers/1801/1801.07883.pdf\n",
        "* Liu (2015): https://www.cs.uic.edu/~liub/FBS/SentimentAnalysis-and-OpinionMining.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SFJy9bei0R3"
      },
      "source": [
        "En este última sección, basandonos en toda la literatura mencionada previamented, se procederá a aplicar un modelo de Deep Learning para el análisis de sentimiento. Para ello, realizaremos lo siguiente:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF2mP60Gilb6"
      },
      "source": [
        "* Carga de datos: Por la manera en que trabajan las librerías de Tensorflow, para este clasificador podremos utilizar la totalidad de las instancias de nuestro dataset de entrenamiento original.\n",
        "* Eliminaremos las instancias cuya puntuación haya sido de 3 estrellas, manteniendo la lógica ya anunciada.\n",
        "* Aplicaremos una función de limpieza más sencilla, que nos permita recorrer las 160 mil instancias más rápidamente.\n",
        "* Aplicaremos Enconding con la función provista por tensorflow datasets\n",
        "* Realizaremos un proceso de Padding, en el que nos aseguramos que todos los tokens tengan la misma longitud. Este proceso en necesario para poder ser utilizado posteriormente por nuestra red neuronal convolucional.\n",
        "* Utilizaremos una red neuronal preentrenada, definiremos las funciones que vamos a utlizar y los parámetros a aplicar.\n",
        "* Finalmente, entrenaremos nuestra red y la evaluaremos, siempre haciendo uso de las funciones de Tensorflow Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRwM23aQdFBe"
      },
      "source": [
        "# Importamos Tensor Flow (descargamos la última de google colab)\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers # para las capas de convolución y las capas densas de keras\n",
        "import tensorflow_datasets as tfds # utilizaremos el tokenizador de tensor flow\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoTkEZDViW5U"
      },
      "source": [
        "train_data = pd.read_json(\"/content/gdrive/MyDrive/dataset_es_train.json\", lines= True)\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2OuJ2Uoik-p"
      },
      "source": [
        "train_data['stars_calif'] = [1 if  train_data['stars'][i]> 3 else 0 for i in train_data.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twxEmON9ine-"
      },
      "source": [
        "# Creo la variable 'review_all', que es una concatenación de 'review_title' y 'review_body'\n",
        "train_data['review_all']=[(str(train_data['review_title'][i])+\" \"+str(train_data['review_body'][i])) for i in train_data.index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUp6o8ccipaL"
      },
      "source": [
        "train_data = train_data[train_data['stars']!=3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwWm6SOEjJvY"
      },
      "source": [
        "train_data.stars_calif.value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sXnMZ1qjL90"
      },
      "source": [
        "data = train_data[['review_all', 'stars_calif']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf5pZsUSjOMv"
      },
      "source": [
        "data.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w70VaS7jQl1"
      },
      "source": [
        "# Defino mi función de limpieza:\n",
        "\n",
        "def clean_review(review):\n",
        "    review = BeautifulSoup(review, \"lxml\").get_text()\n",
        "    # Eliminamos cualquier caracter que no sen los siguientes: a-z A-Z 0-9 signo de admiración o puntuación y espacios\n",
        "    review = re.sub(r\"[^a-zA-Z0-9!?\\\"\\s]\", ' ', review)\n",
        "    # Eliminamos espacios en blanco adicionales\n",
        "    review = re.sub(r\" +\", ' ', review)\n",
        "    return review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4218VPNjUKI"
      },
      "source": [
        "# Aplico la función review por review y obtenemos nuestro corpus. El corpus es la lista de todo el texto que se quiere analizar\n",
        "data_clean = [clean_review(review) for review in data.review_all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvdC6lM2jWfJ"
      },
      "source": [
        "# Defino mi Y\n",
        "data_labels = data.stars_calif.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgKH1mPbj2hJ"
      },
      "source": [
        "len(data_clean)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76nfW0Wnj31t"
      },
      "source": [
        "len(data_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgorWJPhj7Hv"
      },
      "source": [
        "# Vamos a obtener un vector de números y cada uno de llos representará una palabra diferente.\n",
        "# Vamos a tokenizar y vectorizar con el corpues de tensorflow. Construye el tokenizador a base de un corpus.\n",
        "\n",
        "# Definimos el tokenizador:\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(data_clean, target_vocab_size=2**16)\n",
        "\n",
        "# Tokenizamos:\n",
        "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13g9WTKkAFH"
      },
      "source": [
        "len(data_inputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He8os4oFkC_u"
      },
      "source": [
        "# Vamos a querer entrenar por bloques, es decir, por conjuntos de frases. Para esto necesitamos que todos tengan la misma logitud.\n",
        "# El proceso de pading agrega 0 a cada una de esas frases para que todas tengan la misma longitud. Lo hacemos con 0 porque nuestro tokenizador no asigna ese numero a ninguna palabra.\n",
        "\n",
        "# La maxima longitud a ser considerada será la longitud de la frase mas larga que tengamos en nuestro dataset\n",
        "MAX_LEN = max([len(sentence) for sentence in data_inputs])\n",
        "\n",
        "# El pad_sequences nos sirve para añadir algo al principio o al final de una secuencia\n",
        "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
        "                                                            value=0, # Asignamos el 0 anteriormente indicado\n",
        "                                                            padding=\"post\", # Le decimos que sea al final\n",
        "                                                            maxlen=MAX_LEN) # Le pedimos que tenga la máxima longitud de la frase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E61HV_IXkfur"
      },
      "source": [
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(data_inputs, data_labels, test_size=0.2, random_state=42, stratify=data_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BWKjRk6klN9"
      },
      "source": [
        "# Definimos una clase que hereda de una clase de tensor flow - keras - model\n",
        "class DCNN(tf.keras.Model):\n",
        "    # Declaramos el constructor y definimos las capas que van a ser utilizadas\n",
        "    def __init__(self, #hacemos referencia al propio objeto de la clase que va a guardar los parámetros\n",
        "                 # Definimos la lista de parámetros que vamos a utilizar para construir nuestro modelo de red neuronal convolucional\n",
        "                 vocab_size, #tamaño del volabulario\n",
        "                 emb_dim=128, #dimension de emberdding, a qué espacio vectorial vamos a embeber nuestra información. Le pedimos que cada palabra sea resumida a un espacio vectorial de 128 números\n",
        "                 nb_filters=50, #cuántos filtros vamos a utilizar en cada palabra para obtener las correlaciones entre ellas\n",
        "                 FFN_units=512, #numeros de neuronas de la capa oculta\n",
        "                 nb_classes=2, #categorías de clasificación\n",
        "                 dropout_rate=0.1, #es para que ciertas neuronas se desactiven y que no todas aprendan a la vez, es para evitar el overfiting. El 10% de las neuronas no transmitiran lo que han aprendido en la fase de entrenamiento\n",
        "                 training=False, #le indicamos que sólo desactivaremos las neuronas durante la fase de entrenamiento, nunca durante la fase de predicción\n",
        "                 name=\"dcnn\" #le asignamos un nombre al modelo\n",
        "                 ):\n",
        "        # Inicializamos el modelo y hacemos la llamada a la superclase\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        # Deinimos la capa de embeding\n",
        "        self.embedding = layers.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        # Definimos 3 familias de filtros de convolución, que van a analizar 2, 3 y 4 palabras:\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding=\"valid\", activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding=\"valid\", activation=\"relu\")\n",
        "\n",
        "        self.pool = layers.GlobalMaxPool1D() # No tenemos variable de entrenamiento así que podemos usar la misma capa para cada paso de pooling\n",
        "\n",
        "        # Ahora definimos la red neuronal que se va a encargar de la clasificación\n",
        "        # Definimos la capa densa (la capa oculta)\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "\n",
        "        # Capa de dropout para prevenir el overfiting\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        # Capa de salida, última capa densa. La función de activación va a depender de la cantidad de clases a predecir.\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1, activation=\"sigmoid\") # nos va a devolver 0 ó 1\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes, activation=\"softmax\") # nos va a dar las probabilidades reales\n",
        "    \n",
        "    # Creamos la función que se va a utilizar para llamar al modelo\n",
        "    def call(self, inputs, training): # vamos a tener que pasarle las entradas y si estamos o no en la fase de entrenamiento para aplicar el dropout\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x)\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.trigram(x)\n",
        "        x_2 = self.pool(x_2)\n",
        "        x_3 = self.fourgram(x)\n",
        "        x_3 = self.pool(x_3)\n",
        "        \n",
        "        # concatenemos las 4 entradas a la red neuronal\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Aj48zfkwEs"
      },
      "source": [
        "# Definimos los parámetros globales\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocab_size #numero de palabras diferentes que vamos a utilizar\n",
        "\n",
        "EMB_DIM = 200 # Dimension de embeding. Cada palabra se identiicará con un punto de 200 coordenadas\n",
        "NB_FILTERS = 100 # Filtros de la red neuronal convolucional\n",
        "FFN_UNITS = 256 # Numero de unidades que tendrá en la capa oculta\n",
        "NB_CLASSES = 2 #len(set(train_labels))\n",
        "\n",
        "DROPOUT_RATE = 0.2 # Tasa de olvido durante la propagación hacia atrás\n",
        "\n",
        "BATCH_SIZE = 32 # Tamaño del bloque de elementos a entrenar (de 32 en 32 reviews para evitar el overfiting) Es un batch learning\n",
        "NB_EPOCHS = 5 #Numero de veces que vamos a pasar por todo el conjunto de entrenamiento. Vamos a iterar 5 veces sobre todo el dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJTf-AhvkzNd"
      },
      "source": [
        "# Creamos la red neuronal convolucional con los parámetros anteriormente definidos\n",
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcmrTj7Zk2bL"
      },
      "source": [
        "# Compilamos la red en función de la cantidad de clases\n",
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) # Nos devuelve qué porcentaje del texto es correctamente predicho por nuestro modelo\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"]) # Buscamos una colección de números que"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdAEAYlolJbo"
      },
      "source": [
        "# Ajustamos los datos\n",
        "history = Dcnn.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NB_EPOCHS)\n",
        "#ckpt_manager.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRDuNtOVpO4e"
      },
      "source": [
        "Como se puede ver como resultado del último epoch, tenemos un accuracy mayor un poco mayor al 99.5%, mostrando el nivel de preción que tienen estos modelos, aunque lucen un poco overfitteados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdwv61ZwqY3v"
      },
      "source": [
        "Arriba podemos plotear a los accuracy de train y de test y las pérdidas de validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvNKTzkDjqk1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  #plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoxyuAnAlMxk"
      },
      "source": [
        "results_train = Dcnn.evaluate(train_inputs, train_labels, batch_size=BATCH_SIZE)\n",
        "results_test = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
        "print(results_train)\n",
        "print(results_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Za2ogik2rd"
      },
      "source": [
        "### Ejemplos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd9udJMIlpG7"
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"Me llego bien el producto\")]), training=False).numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Qq4rvOkuRZ"
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"El producto es de MMUY buena calidad\")]), training=False).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7lsVwLaiTqX"
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"No lo recomiendo no funciona\")]), training=False).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6fY3sy4k7j0"
      },
      "source": [
        "Dcnn(np.array([tokenizer.encode(\"Esto es un DESASTRE!!!\")]), training=False).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcjrm8Svk6vG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNL40_5DicgG"
      },
      "source": [
        "La performance de el último clasificador obtenido es más que satisfactario. Además de la métrica obtenida, cabe resaltar que la forma en que procesa la infomación tensorflow nos ha permitido procesar la totalidad de las intancias sin inconvenientes de procesamiento, sin necesidad de aplicar una selección de features y sin necesidad de realizar grandes esfuerzos en análisis de limpieza y normalización de los tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjyp5MXNjIAx"
      },
      "source": [
        "**Conclusión**\n",
        "\n",
        "\n",
        "En el presente trabajo, nos propusimos construir un clasificador que prediga la cantidad de estrellas con las que se calificará a un producto a partir de la crítica escrita en la reseña por el usuario.\n",
        "\n",
        "Para lograrlo, se aplicaron técnicas de normalización de texto y vectorización, se probaron diferentes modelos y se realizó una busqueda de hiperparámetros óptimos. Los resultados obtenidos en la predicción de estrellas no lograron superar significativamente los obtenidos al azar, sin embargo, han obtenido grandes mejoras convirtiendo nuestra clasificación en un problema binario.\n",
        "\n",
        "Finalmente, se obtuvieron resultados más satisfactorios aplicado un modelo de deep learning que de machine learning, no sólo por la performance en la clasificación, sino también por el tiempo y capacidad de procesamiento del mismo.\n",
        "\n",
        "Como futuras lineas de investigacion, se podria trabajar en implementar otros modelos para predicción como el modelo BERT. Además, sería recomendable utilizar datos geospaciales de los clientes para ver posibles posibles tendencias en determinadas regionales, que llevan a un mejor uso de los recursos  "
      ]
    }
  ]
}